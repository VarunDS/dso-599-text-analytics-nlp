{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "## The Problem\n",
    "\n",
    "There is an interesting tradeoff between model performance and a feature's dimensionality:\n",
    "![http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/](images/dimensionality_vs_performance.png)\n",
    "\n",
    ">*If the amount of available training data is fixed, then overfitting occurs if we keep adding dimensions. On the other hand, if we keep adding dimensions, the amount of **training data needs to grow exponentially fast to maintain the same coverage** and to avoid overfitting* ([Computer Vision for Dummies](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)).\n",
    "\n",
    "![http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/](images/curseofdimensionality.png)\n",
    "\n",
    "### Multi-Collinearity\n",
    "\n",
    "In many cases, there is a high degree of correlation between many of the features in a dataset. For instance, suppose that you\n",
    "\n",
    "\n",
    "## Sparsity\n",
    "\n",
    "- High dimensionality increases the sparsity of your features (**what NLP techniques have we used that illustrate this point?**)\n",
    "- The density of the training samples decreases when dimensionality increases:\n",
    "- **Distance measures (Euclidean, for instance) start losing their effectiveness**, because there isn't much difference between the max and min distances in higher dimensions.\n",
    "- Many models that rely upon **assumptions of Gaussian distributions** (like OLS linear regression), Gaussian mixture models, Gaussian processes, etc. become less and less effective since their distributions become flatter and \"fatter tailed\".\n",
    "![http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/](images/distance-asymptote.png)\n",
    "\n",
    "What is the amount of data needed to maintain **20% coverage** of the feature space? For 1 dimension, it is **20% of the entire population's dataset**. For a dimensionality of $D$:\n",
    "\n",
    "$$\n",
    "X^{D} = .20\n",
    "$$\n",
    "$$\n",
    "(X^{D})^{\\frac{1}{D}} = .20^{\\frac{1}{D}}\n",
    "$$\n",
    "$$\n",
    "X = \\sqrt[D]{.20}\n",
    "$$\n",
    "You can approximate this as \n",
    "```python\n",
    "def coverage_requirement(requirement, D):\n",
    "    return requirement ** (1 / D)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for d in range(1,20):\n",
    "    y.append(coverage_requirement(0.10, d))\n",
    "    x.append(d)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Number of Dimensions\")\n",
    "plt.ylabel(\"Appromximate % of Population Dataset\")\n",
    "plt.title(\"% of Dataset Needed to Maintain 10% Coverage of Feature Space\")\n",
    "plt.show()\n",
    "```\n",
    "<img src=\"images/coverage-needed.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "reviews = pd.read_csv(\"mcdonalds-yelp-sentiment.csv\", encoding='latin-1')\n",
    "reviews = open(\"poor_amazon_toy_reviews.txt\", encoding='latin-1')\n",
    "\n",
    "#text = reviews[\"review\"].values\n",
    "text = reviews.readlines()\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), min_df=0.01, max_df=0.75, max_features=200)\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "vector = vectorizer.transform(text)\n",
    "features = vector.toarray()\n",
    "features_df = pd.DataFrame(features, columns=vectorizer.get_feature_names())\n",
    "\n",
    "correlations = features_df.corr()\n",
    "correlations_stacked = correlations.stack().reset_index()\n",
    "#set column names\n",
    "correlations_stacked.columns = ['Bi-Gram 1','Bi-Gram 2','Correlation']\n",
    "correlations_stacked = correlations_stacked[correlations_stacked[\"Correlation\"] < 1]\n",
    "correlations_stacked = correlations_stacked.sort_values(by=['Correlation'], ascending=False)\n",
    "correlations_stacked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correlations (install seaborn first)!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "\n",
    "If you have an original matrix $Z$, you can decompose this matrix into two smaller matrices $X$ and $Q$. \n",
    "\n",
    "## Important Points:\n",
    "\n",
    "- Multiplying a vector by a matrix typically changes the direction of the vector. For instance:\n",
    "<figure>\n",
    "  <img src=\"images/multvector.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><a href=\"https://lazyprogrammer.me/tutorial-principal-components-analysis-pca\">Lazy Programmer- \n",
    "        Tutorial to PCA</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are eigenvalues Î» and eigenvectors $v$ such that\n",
    "\n",
    "$$\n",
    "\\sum_{X}v = \\lambda v\n",
    "$$\n",
    "\n",
    "Multiplying the eigenvectors $v$ with the eigenvalue $\\lambda$ does not change the direction of the eigenvector.\n",
    "\n",
    "Multiplying the eigenvector $v$ by the covariance matrix $\\sum_{X}$ also does not change the direction of the eigenvector.\n",
    "\n",
    "If our data $X$ is of shape $N \\times D$, it turns out that we have $D$ eigenvalues and $D$ eigenvectors. This means we can arrange the eigenvalues $\\lambda$ in decreasing order so that\n",
    "\n",
    "$$\n",
    "\\lambda_3 > \\lambda_2 > \\lambda_5\n",
    "$$\n",
    "\n",
    "In this case, $\\lambda_3$ is the largest eigenvalue, followed by $\\lambda_2$, and then $\\lambda_5$. Then, we can arrange \n",
    "\n",
    "We can also rearrange the eigenvectors the same: $v_3$ will be the first column, $v_2$ will be the second column, and $v_5$ will be the third column.\n",
    "\n",
    "We'll end up with two matrices $V$ and $\\Lambda$:\n",
    "<figure>\n",
    "  <img src=\"images/pca1.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><a href=\"https://lazyprogrammer.me/tutorial-principal-components-analysis-pca\">Lazy Programmer- \n",
    "        Tutorial to PCA</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape of our features?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# what is the shape of Z?\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what will happen if we take the correlation matrix and covariance matrix of our new reduced features?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model to reduce the dimensions down to 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "Given an input matrix $A$, we want to try to represent it instead as three smaller matrices $U$, $\\sum$, and $V$. Instead of **$n$ original terms**, we want to represent each document as **$r$ concepts** (other referred to as **latent dimensions**, or **latent factors**):\n",
    "<figure>\n",
    "  <img src=\"images/svd.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>\n",
    "        <a href=\"https://www.youtube.com/watch?v=P5mlg91as1c\">Mining of Massive Datasets - Dimensionality Reduction: Singular Value Decomposition</a> by Leskovec, Rajaraman, and Ullman (Stanford University)</i></figcaption>\n",
    "</figure>\n",
    "\n",
    "Here, **$A$ is your matrix of word vectors** - you could use any of the word vectorization techniques we have learned so far, include one-hot encoding, word count, TF-IDF.\n",
    "\n",
    "- $\\sum$ will be a **diagonal matrix** with values that are positive and sorted in decreasing order. Its value indicate the **variance (information encoded on that new dimension)**- therefore, the higher the value, the stronger that dimension is in capturing data from A, the original features. For our purposes, we can think of the rank of this $\\sum$ matrix as the number of desired dimensions. Instance, if we want to reduce $A$ from shape $1020 x 300$ to $1020 x 10$, we will want to reduce the rank of $\\sum$ from 300 to 10.\n",
    "\n",
    "- $U^T U = I$ and $V^T V = I$\n",
    "\n",
    "## Measuring the Quality of the Reconstruction\n",
    "\n",
    "A popular metric used for measuring the quality of the reconstruction is the [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm). When you explain your methodology for reducing dimensions, usually managers / stakeholders will want to some way to compare multiple dimensionality techniques' ability to quantify its ability to retain information but trim dimensions:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "||A_{old}-A_{new}||_{F} = \\sqrt{\\sum_{ij}{(A^{old}_{ij}- A^{new}_{ij}}})^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## Heuristic Step for How Many Dimensions to Keep\n",
    "\n",
    "1. Sum the $\\sum$ matrix's diagonal values: \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sum_{i}^{m}\\sigma_{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "2. Define your threshold of \"information\" (variance) $\\alpha$ to keep: usually 80% to 90%. \n",
    "\n",
    "3. Define your cutoff point $C$: $$\n",
    "\\begin{equation}\n",
    "C = \\sum_{i}^{m}\\sigma_{i} \\alpha\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "4. Beginning with your largest singular value, sum your singular values $\\sigma_{i}$ until it is greater than C. Retain only those dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/userratings.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>\n",
    "        <a href=\"https://www.youtube.com/watch?v=P5mlg91as1c\">Mining of Massive Datasets - Dimensionality Reduction: Singular Value Decomposition</a> by Leskovec, Rajaraman, and Ullman (Stanford University)</i></figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frobenius norm\n",
    "\n",
    "\n",
    "\n",
    "# reconstruct matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOVE\n",
    "\n",
    "Global vectors for word presentation:\n",
    "<figure>\n",
    "  <img src=\"images/glove_1.png\" alt=\"my alt text\"/>\n",
    "    <figcaption><i>\n",
    "        <a href=\"https://nlp.stanford.edu/pubs/glove.pdf\">GloVe: Global Vectors for Word Representation</a></i></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import glove embeddings into a word2vec format that is consumable by Gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# calculate: (king - man) + woman = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"woman\", \"king\", \"man\", \"queen\", \"puppy\", \"kitten\", \"cat\", \n",
    "         \"quarterback\", \"football\", \"stadium\", \"touchdown\",\n",
    "         \"dog\", \"government\", \"tax\", \"federal\", \"judicial\", \"elections\",\n",
    "         \"avocado\", \"tomato\", \"pear\", \"championship\", \"playoffs\"]\n",
    "\n",
    "# load word vectors in Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# use variety of dimensionality techniques to reduce dimensionality\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {\n",
    "    0: \"r\",\n",
    "    1: \"b\",\n",
    "    2: \"g\",\n",
    "    3: \"y\"\n",
    "}\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise (20 minutes)\n",
    "\n",
    "1. Use the **Amazon Fire dataset from the midterm** for the following exercise - vectorize the data using either Gensim or Spacy embeddings, and then reduce the dimensionality down to 2 dimensions\n",
    "2. Visualize the results and see if you can identify common patterns in the clusters.\n",
    "\n",
    "**Challenge:** In the example we have just done, we modelled the relationship between `x` and `y` as a linear relationship. Your manager does not think SVD works as well with non-linear relationships, and is skeptical because many of the social media marketing data she sees has non-linear relationships (ie. the number of times a post was seen versus the number of reactions it got). Investigate what the impact of non-linear relationships on SVD performance is in terms of:\n",
    "\n",
    "- **# of dimensions needed to capture 90% of the variance**\n",
    "- **Frobenius Norm** reconstruction error\n",
    "\n",
    "If SVD is not significantly affected by non-linearity, **explain to your manager why**.\n",
    "\n",
    "If SVD is significantly affected, **evaluate whether or not SVD can still be used as a dimensionality reduction technique**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
