{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Example\n",
    "\n",
    "### Intro to Algorithmic Marketing:\n",
    "![alt text](images/cos-sim-textbook1.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Magnitude of a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First approach: 3.7416573867739413\n",
      "Second approach: 3.7416573867739413\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def magnitude(x): \n",
    "    return math.sqrt(sum(i**2 for i in x))\n",
    "\n",
    "vectorA = [0,3,1,2]\n",
    "\n",
    "print(f\"First approach: {magnitude(vectorA)}\")\n",
    "print(f\"Second approach: {np.linalg.norm(vectorA)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Count Vectorization - One Hot Encoding and other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (655, 23148)\n",
      "Total number of occurences: 402534\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "plots_df = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "# filter only for American movies\n",
    "plots_df = plots_df[plots_df[\"Origin/Ethnicity\"] == \"American\"]\n",
    "\n",
    " # traditional CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    " # use English stopwords, and use one-hot encoding\n",
    "#vectorizer = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "#vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "#vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "# vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "X = vectorizer.fit_transform(plots_df[\"Plot\"])\n",
    "\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {vectorized_df.shape}\")\n",
    "print(f\"Total number of occurences: {vectorized_df.sum().sum()}\")\n",
    "#print(f\"Word counts: {vectorized_df.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "articles = [f\"bbcsport/football/00{i}.txt\" for i in range(1,10)]\n",
    "\n",
    "for article in articles:\n",
    "    article = open(article) # open each sports article\n",
    "    for line in article.readlines():\n",
    "        line = line.replace(\"\\n\", \"\") # replace the new line escape character\n",
    "        if len(line) > 0: # if the line is not empty, process it\n",
    "            line = [lemmatizer.lemmatize(token) for token in word_tokenize(line)] \n",
    "            documents.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "for doc in documents:\n",
    "    new_document = []\n",
    "    for word in doc:\n",
    "        if word.strip().lower() not in stopwords:\n",
    "            new_document.append(word)\n",
    "    new_documents.append(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Champions', 'League'),\n",
       " ('Manchester', 'United'),\n",
       " ('Cristiano', 'Ronaldo'),\n",
       " ('Van', 'Nistelrooy'),\n",
       " ('Wayne', 'Rooney'),\n",
       " ('Alex', 'Ferguson'),\n",
       " ('FA', 'Cup'),\n",
       " ('Ferguson', 'wa'),\n",
       " ('Gary', 'Neville'),\n",
       " ('Man', 'Utd'),\n",
       " ('Manchester', 'City'),\n",
       " ('Sir', 'Alex'),\n",
       " ('national', 'team'),\n",
       " ('wa', \"n't\"),\n",
       " ('23', 'minute'),\n",
       " ('BBC', 'Radio'),\n",
       " ('Blues', 'bos'),\n",
       " ('Carling', 'Cup'),\n",
       " ('Carroll', 'Gary'),\n",
       " ('City', 'Sunday'),\n",
       " ('City', 'best'),\n",
       " ('Cup', 'final'),\n",
       " ('Five', 'Live'),\n",
       " ('Gallas', 'would'),\n",
       " ('Gerrard', 'ha'),\n",
       " ('Goodison', 'Park'),\n",
       " ('Home', 'International'),\n",
       " ('International', 'series'),\n",
       " ('Jose', 'Mourinho'),\n",
       " ('Man', 'City')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocation_finder = BigramCollocationFinder.from_documents(new_documents)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "It's important to identify a **context window** when analyzing co-occurence. In the image below, the context window size is 4 (2 tokens to either side of the target word):\n",
    "\n",
    "![alt text](images/context_window.png \"Logo Title Text 1\")\n",
    "\n",
    "For the purposes of the next section, we'll define the **entire document as the context window.**\n",
    "\n",
    "Pointwise mutual information measures the ratio between the **joint probability of two events happening** with the probabilities of the two events happening, assuming they are independent. It can be defined with the following equation:\n",
    "\n",
    "$$\n",
    "PMI_{A,B} = log\\frac{p(A,B)}{p(A)p(B)}\n",
    "$$\n",
    "\n",
    "Remember that when two events are independent, $P(i,j) = P(i)P(j)$. Using PMI to just a raw word count is often preferable because very common words have extreme skew (\"the\" and \"of\" will co-occur frequently in the same  )\n",
    "\n",
    "```python\n",
    "import math\n",
    "def pmi(tokenA, tokenB, documents, word_counts):\n",
    "    \n",
    "    # word_counts[token_A] => number of times tokenA appears in the documents\n",
    "    # float(len(documents)) = number of documents\n",
    "    \n",
    "    prob_A = word_counts[tokenA] / float(len(documents))\n",
    "    prob_B = word_counts[tokenB] / float(len(documents))\n",
    "    prob_A_B = bigram_freq[\" \".join([word1, word2])] / float(len(documents))\n",
    "    return math.log(prob_A_B/float(prob_A*prob_B),2) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency / Inverse Document Frequency\n",
    "\n",
    "\n",
    "## Term Frequency\n",
    "![alt text](images/tf-idf1.png \"Term Frequency\")\n",
    "\n",
    "## Inverse Document Frequency\n",
    "![alt text](images/tf-idf2.png \"Inverse Document Frequency\")\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "![alt text](images/tf-idf4.png \"Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,3),\n",
    "                             token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "                             max_df=0.4,\n",
    "                             min_df=1, stop_words=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"mcdonalds-yelp-negative-reviews.csv\", encoding=\"latin1\")\n",
    "corpus = list(df[\"review\"].values)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "tf_idf = tf_idf.sum(axis=1)\n",
    "score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "score[\"term\"] = terms\n",
    "score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>drive thru</th>\n",
       "      <td>17.022681</td>\n",
       "      <td>drive thru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fast food</th>\n",
       "      <td>7.435900</td>\n",
       "      <td>fast food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>customer service</th>\n",
       "      <td>6.854534</td>\n",
       "      <td>customer service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ice cream</th>\n",
       "      <td>4.691034</td>\n",
       "      <td>ice cream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get order</th>\n",
       "      <td>4.554700</td>\n",
       "      <td>get order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst mcdonalds</th>\n",
       "      <td>4.440882</td>\n",
       "      <td>worst mcdonalds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order wrong</th>\n",
       "      <td>4.065942</td>\n",
       "      <td>order wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcdonald ever</th>\n",
       "      <td>3.661332</td>\n",
       "      <td>mcdonald ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big mac</th>\n",
       "      <td>3.617849</td>\n",
       "      <td>big mac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every time</th>\n",
       "      <td>3.543326</td>\n",
       "      <td>every time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mcdonalds ever</th>\n",
       "      <td>3.484545</td>\n",
       "      <td>mcdonalds ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst mcdonald</th>\n",
       "      <td>3.410093</td>\n",
       "      <td>worst mcdonald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>order right</th>\n",
       "      <td>3.019427</td>\n",
       "      <td>order right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>late night</th>\n",
       "      <td>2.806158</td>\n",
       "      <td>late night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken nuggets</th>\n",
       "      <td>2.739145</td>\n",
       "      <td>chicken nuggets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parking lot</th>\n",
       "      <td>2.693634</td>\n",
       "      <td>parking lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst mcdonald ever</th>\n",
       "      <td>2.614559</td>\n",
       "      <td>worst mcdonald ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get food</th>\n",
       "      <td>2.604047</td>\n",
       "      <td>get food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good service</th>\n",
       "      <td>2.503009</td>\n",
       "      <td>good service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even though</th>\n",
       "      <td>2.417658</td>\n",
       "      <td>even though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get order right</th>\n",
       "      <td>2.398593</td>\n",
       "      <td>get order right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst mcdonalds ever</th>\n",
       "      <td>2.349002</td>\n",
       "      <td>worst mcdonalds ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went drive</th>\n",
       "      <td>2.312716</td>\n",
       "      <td>went drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minutes get</th>\n",
       "      <td>2.304963</td>\n",
       "      <td>minutes get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service ever</th>\n",
       "      <td>2.215430</td>\n",
       "      <td>service ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first time</th>\n",
       "      <td>2.132917</td>\n",
       "      <td>first time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iced coffee</th>\n",
       "      <td>2.127732</td>\n",
       "      <td>iced coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french fries</th>\n",
       "      <td>2.084945</td>\n",
       "      <td>french fries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sweet tea</th>\n",
       "      <td>2.065265</td>\n",
       "      <td>sweet tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last time</th>\n",
       "      <td>2.015504</td>\n",
       "      <td>last time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cases occasionally</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>cases occasionally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pressed tortilla feathering</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>pressed tortilla feathering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back might judge</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>back might judge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure knowing</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>sure knowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure knowing luck</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>sure knowing luck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back might</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>back might</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guy front washed</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>guy front washed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noooooooo mccrap</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>noooooooo mccrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>noooooooo mccrap totally</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>noooooooo mccrap totally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second mcdonald</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>second mcdonald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second mcdonald review</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>second mcdonald review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serving swedish meatballs</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>serving swedish meatballs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>folks left line</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>folks left line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>folks left</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>folks left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serving swedish</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>serving swedish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewing mcdonald resides</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>reviewing mcdonald resides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perhaps ripening</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>perhaps ripening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second round</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>second round</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second round thing</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>second round thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perhaps ripening tomato</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>perhaps ripening tomato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soggy way long</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>soggy way long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soggy way</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>soggy way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soggy upon mcwrap</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>soggy upon mcwrap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soggy upon</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>soggy upon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know almost</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>know almost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know almost summer</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>know almost summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know damn</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>know damn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>execs change</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>execs change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>know damn fry</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>know damn fry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peeking top</th>\n",
       "      <td>0.033689</td>\n",
       "      <td>peeking top</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116571 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 score                         term\n",
       "drive thru                   17.022681                   drive thru\n",
       "fast food                     7.435900                    fast food\n",
       "customer service              6.854534             customer service\n",
       "ice cream                     4.691034                    ice cream\n",
       "get order                     4.554700                    get order\n",
       "worst mcdonalds               4.440882              worst mcdonalds\n",
       "order wrong                   4.065942                  order wrong\n",
       "mcdonald ever                 3.661332                mcdonald ever\n",
       "big mac                       3.617849                      big mac\n",
       "every time                    3.543326                   every time\n",
       "mcdonalds ever                3.484545               mcdonalds ever\n",
       "worst mcdonald                3.410093               worst mcdonald\n",
       "order right                   3.019427                  order right\n",
       "late night                    2.806158                   late night\n",
       "chicken nuggets               2.739145              chicken nuggets\n",
       "parking lot                   2.693634                  parking lot\n",
       "worst mcdonald ever           2.614559          worst mcdonald ever\n",
       "get food                      2.604047                     get food\n",
       "good service                  2.503009                 good service\n",
       "even though                   2.417658                  even though\n",
       "get order right               2.398593              get order right\n",
       "worst mcdonalds ever          2.349002         worst mcdonalds ever\n",
       "went drive                    2.312716                   went drive\n",
       "minutes get                   2.304963                  minutes get\n",
       "service ever                  2.215430                 service ever\n",
       "first time                    2.132917                   first time\n",
       "iced coffee                   2.127732                  iced coffee\n",
       "french fries                  2.084945                 french fries\n",
       "sweet tea                     2.065265                    sweet tea\n",
       "last time                     2.015504                    last time\n",
       "...                                ...                          ...\n",
       "cases occasionally            0.033689           cases occasionally\n",
       "pressed tortilla feathering   0.033689  pressed tortilla feathering\n",
       "back might judge              0.033689             back might judge\n",
       "sure knowing                  0.033689                 sure knowing\n",
       "sure knowing luck             0.033689            sure knowing luck\n",
       "back might                    0.033689                   back might\n",
       "guy front washed              0.033689             guy front washed\n",
       "noooooooo mccrap              0.033689             noooooooo mccrap\n",
       "noooooooo mccrap totally      0.033689     noooooooo mccrap totally\n",
       "second mcdonald               0.033689              second mcdonald\n",
       "second mcdonald review        0.033689       second mcdonald review\n",
       "serving swedish meatballs     0.033689    serving swedish meatballs\n",
       "folks left line               0.033689              folks left line\n",
       "folks left                    0.033689                   folks left\n",
       "serving swedish               0.033689              serving swedish\n",
       "reviewing mcdonald resides    0.033689   reviewing mcdonald resides\n",
       "perhaps ripening              0.033689             perhaps ripening\n",
       "second round                  0.033689                 second round\n",
       "second round thing            0.033689           second round thing\n",
       "perhaps ripening tomato       0.033689      perhaps ripening tomato\n",
       "soggy way long                0.033689               soggy way long\n",
       "soggy way                     0.033689                    soggy way\n",
       "soggy upon mcwrap             0.033689            soggy upon mcwrap\n",
       "soggy upon                    0.033689                   soggy upon\n",
       "know almost                   0.033689                  know almost\n",
       "know almost summer            0.033689           know almost summer\n",
       "know damn                     0.033689                    know damn\n",
       "execs change                  0.033689                 execs change\n",
       "know damn fry                 0.033689                know damn fry\n",
       "peeking top                   0.033689                  peeking top\n",
       "\n",
       "[116571 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
