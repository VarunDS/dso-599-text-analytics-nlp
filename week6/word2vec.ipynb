{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "X = data.loc[:, ~data.columns.isin(['TARGET'])]\n",
    "cv_results = cross_validate(lr, X, y, cv=10,return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9550992 , 0.95475113, 0.95744866, 0.95544727, 0.95475113,\n",
       "       0.95857988, 0.95466411, 0.95570446, 0.95709686, 0.95561744])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings (word2vec Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag of Words (Use Context to Predict Target Word)\n",
    "![alt text](images/word2vec_cbow.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "![alt text](images/softmax.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram\n",
    "![alt text](images/skipgram.png \"Logo Title Text 1\")\n",
    "\n",
    "## Softmax\n",
    "![alt text](images/wordembedding_cluster.png \"Logo Title Text 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dog - cat: 0.4564264118671417\n",
      " dog - Beijing: 0.1571345329284668\n",
      " dog - sad: 0.3079860210418701\n",
      " dog - depressed: 0.11385080963373184\n",
      " dog - couch: 0.5404482483863831\n",
      " dog - sofa: 0.33240464329719543\n",
      " dog - canine: 0.4633784294128418\n",
      " dog - China: 0.0019485866650938988\n",
      " dog - Chinese: 0.021737948060035706\n",
      " dog - France: 0.1857185661792755\n",
      " dog - Paris: 0.11601343750953674\n",
      " dog - banana: 0.3103766441345215\n",
      " cat - dog: 0.4564264118671417\n",
      " cat - Beijing: 0.25583046674728394\n",
      " cat - sad: 0.06742441654205322\n",
      " cat - depressed: 0.11650095880031586\n",
      " cat - couch: 0.37735462188720703\n",
      " cat - sofa: 0.414833128452301\n",
      " cat - canine: 0.45437631011009216\n",
      " cat - China: 0.14348067343235016\n",
      " cat - Chinese: 0.03203266113996506\n",
      " cat - France: 0.26350462436676025\n",
      " cat - Paris: 0.1825326830148697\n",
      " cat - banana: 0.4973468482494354\n",
      " Beijing - dog: 0.1571345329284668\n",
      " Beijing - cat: 0.25583046674728394\n",
      " Beijing - sad: 0.16756749153137207\n",
      " Beijing - depressed: 0.020596839487552643\n",
      " Beijing - couch: 0.1764748990535736\n",
      " Beijing - sofa: 0.16452562808990479\n",
      " Beijing - canine: 0.26767805218696594\n",
      " Beijing - China: 0.5818219780921936\n",
      " Beijing - Chinese: 0.27865567803382874\n",
      " Beijing - France: 0.5205764174461365\n",
      " Beijing - Paris: 0.424411803483963\n",
      " Beijing - banana: 0.3193724453449249\n",
      " sad - dog: 0.3079860210418701\n",
      " sad - cat: 0.06742441654205322\n",
      " sad - Beijing: 0.16756749153137207\n",
      " sad - depressed: 0.3213621973991394\n",
      " sad - couch: 0.35177069902420044\n",
      " sad - sofa: 0.27763885259628296\n",
      " sad - canine: 0.2469976246356964\n",
      " sad - China: 0.14939850568771362\n",
      " sad - Chinese: 0.20662599802017212\n",
      " sad - France: 0.17732539772987366\n",
      " sad - Paris: 0.048603419214487076\n",
      " sad - banana: 0.27784866094589233\n",
      " depressed - dog: 0.11385080963373184\n",
      " depressed - cat: 0.11650095880031586\n",
      " depressed - Beijing: 0.020596839487552643\n",
      " depressed - sad: 0.3213621973991394\n",
      " depressed - couch: 0.2007930874824524\n",
      " depressed - sofa: 0.3122972249984741\n",
      " depressed - canine: 0.1812627762556076\n",
      " depressed - China: 0.1365271508693695\n",
      " depressed - Chinese: 0.21596072614192963\n",
      " depressed - France: 0.07575459033250809\n",
      " depressed - Paris: 0.027627602219581604\n",
      " depressed - banana: 0.0631336122751236\n",
      " couch - dog: 0.5404482483863831\n",
      " couch - cat: 0.37735462188720703\n",
      " couch - Beijing: 0.1764748990535736\n",
      " couch - sad: 0.35177069902420044\n",
      " couch - depressed: 0.2007930874824524\n",
      " couch - sofa: 0.4999978840351105\n",
      " couch - canine: 0.41085895895957947\n",
      " couch - China: 0.08625976741313934\n",
      " couch - Chinese: 0.21824336051940918\n",
      " couch - France: 0.2151881605386734\n",
      " couch - Paris: 0.21405968070030212\n",
      " couch - banana: 0.28344547748565674\n",
      " sofa - dog: 0.33240464329719543\n",
      " sofa - cat: 0.414833128452301\n",
      " sofa - Beijing: 0.16452562808990479\n",
      " sofa - sad: 0.27763885259628296\n",
      " sofa - depressed: 0.3122972249984741\n",
      " sofa - couch: 0.4999978840351105\n",
      " sofa - canine: 0.26518216729164124\n",
      " sofa - China: 0.12562435865402222\n",
      " sofa - Chinese: 0.11188281327486038\n",
      " sofa - France: 0.25162801146507263\n",
      " sofa - Paris: 0.17580696940422058\n",
      " sofa - banana: 0.40957292914390564\n",
      " canine - dog: 0.4633784294128418\n",
      " canine - cat: 0.45437631011009216\n",
      " canine - Beijing: 0.26767805218696594\n",
      " canine - sad: 0.2469976246356964\n",
      " canine - depressed: 0.1812627762556076\n",
      " canine - couch: 0.41085895895957947\n",
      " canine - sofa: 0.26518216729164124\n",
      " canine - China: 0.21173277497291565\n",
      " canine - Chinese: 0.1380724459886551\n",
      " canine - France: 0.27453306317329407\n",
      " canine - Paris: 0.13104529678821564\n",
      " canine - banana: 0.279147207736969\n",
      " China - dog: 0.0019485866650938988\n",
      " China - cat: 0.14348067343235016\n",
      " China - Beijing: 0.5818219780921936\n",
      " China - sad: 0.14939850568771362\n",
      " China - depressed: 0.1365271508693695\n",
      " China - couch: 0.08625976741313934\n",
      " China - sofa: 0.12562435865402222\n",
      " China - canine: 0.21173277497291565\n",
      " China - Chinese: 0.545533299446106\n",
      " China - France: 0.673493504524231\n",
      " China - Paris: 0.4618699848651886\n",
      " China - banana: 0.1769627034664154\n",
      " Chinese - dog: 0.021737948060035706\n",
      " Chinese - cat: 0.03203266113996506\n",
      " Chinese - Beijing: 0.27865567803382874\n",
      " Chinese - sad: 0.20662599802017212\n",
      " Chinese - depressed: 0.21596072614192963\n",
      " Chinese - couch: 0.21824336051940918\n",
      " Chinese - sofa: 0.11188281327486038\n",
      " Chinese - canine: 0.1380724459886551\n",
      " Chinese - China: 0.545533299446106\n",
      " Chinese - France: 0.4462940990924835\n",
      " Chinese - Paris: 0.554480791091919\n",
      " Chinese - banana: 0.03465459123253822\n",
      " France - dog: 0.1857185661792755\n",
      " France - cat: 0.26350462436676025\n",
      " France - Beijing: 0.5205764174461365\n",
      " France - sad: 0.17732539772987366\n",
      " France - depressed: 0.07575459033250809\n",
      " France - couch: 0.2151881605386734\n",
      " France - sofa: 0.25162801146507263\n",
      " France - canine: 0.27453306317329407\n",
      " France - China: 0.673493504524231\n",
      " France - Chinese: 0.4462940990924835\n",
      " France - Paris: 0.3865208923816681\n",
      " France - banana: 0.3307388424873352\n",
      " Paris - dog: 0.11601343750953674\n",
      " Paris - cat: 0.1825326830148697\n",
      " Paris - Beijing: 0.424411803483963\n",
      " Paris - sad: 0.048603419214487076\n",
      " Paris - depressed: 0.027627602219581604\n",
      " Paris - couch: 0.21405968070030212\n",
      " Paris - sofa: 0.17580696940422058\n",
      " Paris - canine: 0.13104529678821564\n",
      " Paris - China: 0.4618699848651886\n",
      " Paris - Chinese: 0.554480791091919\n",
      " Paris - France: 0.3865208923816681\n",
      " Paris - banana: 0.2538565695285797\n",
      " banana - dog: 0.3103766441345215\n",
      " banana - cat: 0.4973468482494354\n",
      " banana - Beijing: 0.3193724453449249\n",
      " banana - sad: 0.27784866094589233\n",
      " banana - depressed: 0.0631336122751236\n",
      " banana - couch: 0.28344547748565674\n",
      " banana - sofa: 0.40957292914390564\n",
      " banana - canine: 0.279147207736969\n",
      " banana - China: 0.1769627034664154\n",
      " banana - Chinese: 0.03465459123253822\n",
      " banana - France: 0.3307388424873352\n",
      " banana - Paris: 0.2538565695285797\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp(u'dog cat Beijing sad depressed couch sofa canine China Chinese France Paris banana')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        if token1 != token2:\n",
    "            print(f\" {token1} - {token2}: {1 - cosine(token1.vector, token2.vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Most Similar Words (Using Our Old Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the default settings for CountVectorizer\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>actual</th>\n",
       "      <th>actually</th>\n",
       "      <th>adult</th>\n",
       "      <th>advertised</th>\n",
       "      <th>ago</th>\n",
       "      <th>air</th>\n",
       "      <th>amazon</th>\n",
       "      <th>apart</th>\n",
       "      <th>...</th>\n",
       "      <th>working</th>\n",
       "      <th>works</th>\n",
       "      <th>worse</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>wouldn</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  absolutely  actual  actually  adult  advertised  ago  air  amazon  \\\n",
       "0     0           0       0         0      0           0    0    0       0   \n",
       "1     0           0       0         0      0           0    0    0       0   \n",
       "2     0           0       0         0      0           0    0    0       0   \n",
       "3     0           0       0         0      0           0    0    0       0   \n",
       "4     0           0       0         0      0           1    0    0       0   \n",
       "\n",
       "   apart  ...   working  works  worse  worst  worth  wouldn  wrong  year  \\\n",
       "0      0  ...         0      0      0      0      0       0      0     0   \n",
       "1      0  ...         0      0      0      0      0       0      0     0   \n",
       "2      0  ...         0      0      0      0      0       0      0     0   \n",
       "3      0  ...         0      0      0      0      0       0      0     0   \n",
       "4      0  ...         0      0      0      0      0       0      0     0   \n",
       "\n",
       "   years  zero  \n",
       "0      0     0  \n",
       "1      0     0  \n",
       "2      0     0  \n",
       "3      0     0  \n",
       "4      0     0  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = open(\"poor_amazon_toy_reviews.txt\").readlines()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
    "                             stop_words=\"english\", \n",
    "                             max_features=500,token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b')\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "data = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create similiarity matrix\n",
    "similarity_matrix = pd.DataFrame(cosine_similarity(data.T.values), \n",
    "             columns=vectorizer.get_feature_names(),\n",
    "                                 index=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack matrix into table\n",
    "similarity_table = similarity_matrix.rename_axis(None).rename_axis(None, axis=1).stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 3)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "similarity_table.columns = [\"word1\", \"word2\", \"similarity\"]\n",
    "similarity_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249500, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_table = similarity_table[similarity_table[\"similarity\"] < 0.99]\n",
    "similarity_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144497</th>\n",
       "      <td>old</td>\n",
       "      <td>year</td>\n",
       "      <td>0.754569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194593</th>\n",
       "      <td>service</td>\n",
       "      <td>customer</td>\n",
       "      <td>0.734095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237276</th>\n",
       "      <td>waste</td>\n",
       "      <td>money</td>\n",
       "      <td>0.655483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245419</th>\n",
       "      <td>working</td>\n",
       "      <td>stopped</td>\n",
       "      <td>0.589414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74650</th>\n",
       "      <td>figure</td>\n",
       "      <td>figures</td>\n",
       "      <td>0.553856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5949</th>\n",
       "      <td>arm</td>\n",
       "      <td>train</td>\n",
       "      <td>0.500379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172826</th>\n",
       "      <td>quality</td>\n",
       "      <td>poor</td>\n",
       "      <td>0.469081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179083</th>\n",
       "      <td>remote</td>\n",
       "      <td>control</td>\n",
       "      <td>0.456930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210121</th>\n",
       "      <td>store</td>\n",
       "      <td>dollar</td>\n",
       "      <td>0.426935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>apart</td>\n",
       "      <td>fell</td>\n",
       "      <td>0.385922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2  similarity\n",
       "144497      old      year    0.754569\n",
       "194593  service  customer    0.734095\n",
       "237276    waste     money    0.655483\n",
       "245419  working   stopped    0.589414\n",
       "74650    figure   figures    0.553856\n",
       "5949        arm     train    0.500379\n",
       "172826  quality      poor    0.469081\n",
       "179083   remote   control    0.456930\n",
       "210121    store    dollar    0.426935\n",
       "4648      apart      fell    0.385922"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_table.sort_values(by=\"similarity\", ascending=False).drop_duplicates(\n",
    "    subset=\"similarity\", keep=\"first\").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500_words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Aside: Named Entity Recognition With Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steve Jobs 0 10 PERSON\n",
      "Apple 15 20 ORG\n",
      "U.K. 42 46 GPE\n",
      "$1 billion 59 69 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"Steve Jobs and Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Similar Words Using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into spacy your top 500 words\n",
    "\n",
    "tokens = nlp(f'{\" \".join(top_500_words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "# create a list of similarity tuples\n",
    "\n",
    "similarity_tuples = []\n",
    "\n",
    "for token1, token2 in product(tokens, repeat=2):\n",
    "    similarity_tuples.append((token1, token2, token1.similarity(token2)))\n",
    "\n",
    "similarities = pd.DataFrame(similarity_tuples, columns=[\"word1\",\"word2\", \"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108082</th>\n",
       "      <td>inches</td>\n",
       "      <td>figures</td>\n",
       "      <td>0.833207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76607</th>\n",
       "      <td>figures</td>\n",
       "      <td>packs</td>\n",
       "      <td>0.825721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211253</th>\n",
       "      <td>stop</td>\n",
       "      <td>start</td>\n",
       "      <td>0.816715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48523</th>\n",
       "      <td>damaged</td>\n",
       "      <td>popped</td>\n",
       "      <td>0.812956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108233</th>\n",
       "      <td>inches</td>\n",
       "      <td>packs</td>\n",
       "      <td>0.812329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32093</th>\n",
       "      <td>charged</td>\n",
       "      <td>used</td>\n",
       "      <td>0.811088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158221</th>\n",
       "      <td>pictured</td>\n",
       "      <td>cracked</td>\n",
       "      <td>0.810884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227883</th>\n",
       "      <td>tried</td>\n",
       "      <td>wasted</td>\n",
       "      <td>0.809038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227873</th>\n",
       "      <td>tried</td>\n",
       "      <td>used</td>\n",
       "      <td>0.806167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112469</th>\n",
       "      <td>items</td>\n",
       "      <td>balloons</td>\n",
       "      <td>0.804584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241741</th>\n",
       "      <td>week</td>\n",
       "      <td>month</td>\n",
       "      <td>0.804216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31722</th>\n",
       "      <td>charged</td>\n",
       "      <td>damaged</td>\n",
       "      <td>0.802467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147409</th>\n",
       "      <td>opened</td>\n",
       "      <td>played</td>\n",
       "      <td>0.798489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227729</th>\n",
       "      <td>tried</td>\n",
       "      <td>played</td>\n",
       "      <td>0.798329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140156</th>\n",
       "      <td>month</td>\n",
       "      <td>day</td>\n",
       "      <td>0.797825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227778</th>\n",
       "      <td>tried</td>\n",
       "      <td>ripped</td>\n",
       "      <td>0.796584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48491</th>\n",
       "      <td>damaged</td>\n",
       "      <td>overpriced</td>\n",
       "      <td>0.796558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227721</th>\n",
       "      <td>tried</td>\n",
       "      <td>pictured</td>\n",
       "      <td>0.793427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106552</th>\n",
       "      <td>immediately</td>\n",
       "      <td>easily</td>\n",
       "      <td>0.792001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45745</th>\n",
       "      <td>cracked</td>\n",
       "      <td>charged</td>\n",
       "      <td>0.788275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31919</th>\n",
       "      <td>charged</td>\n",
       "      <td>opened</td>\n",
       "      <td>0.788022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211756</th>\n",
       "      <td>stopped</td>\n",
       "      <td>started</td>\n",
       "      <td>0.787957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52773</th>\n",
       "      <td>described</td>\n",
       "      <td>charged</td>\n",
       "      <td>0.786303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227699</th>\n",
       "      <td>tried</td>\n",
       "      <td>opened</td>\n",
       "      <td>0.782267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16968</th>\n",
       "      <td>bigger</td>\n",
       "      <td>smaller</td>\n",
       "      <td>0.779627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158193</th>\n",
       "      <td>pictured</td>\n",
       "      <td>charged</td>\n",
       "      <td>0.778561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234727</th>\n",
       "      <td>used</td>\n",
       "      <td>opened</td>\n",
       "      <td>0.778471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218158</th>\n",
       "      <td>terrible</td>\n",
       "      <td>old</td>\n",
       "      <td>0.777368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181787</th>\n",
       "      <td>replaced</td>\n",
       "      <td>charged</td>\n",
       "      <td>0.775764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140557</th>\n",
       "      <td>month</td>\n",
       "      <td>year</td>\n",
       "      <td>0.775223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3481</th>\n",
       "      <td>ago</td>\n",
       "      <td>uses</td>\n",
       "      <td>-0.251178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217435</th>\n",
       "      <td>tell</td>\n",
       "      <td>children</td>\n",
       "      <td>-0.253195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236443</th>\n",
       "      <td>ve</td>\n",
       "      <td>absolutely</td>\n",
       "      <td>-0.253766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62050</th>\n",
       "      <td>dollars</td>\n",
       "      <td>paid</td>\n",
       "      <td>-0.253809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7147</th>\n",
       "      <td>away</td>\n",
       "      <td>doesn</td>\n",
       "      <td>-0.254330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62442</th>\n",
       "      <td>don</td>\n",
       "      <td>hard</td>\n",
       "      <td>-0.254766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118596</th>\n",
       "      <td>later</td>\n",
       "      <td>don</td>\n",
       "      <td>-0.255086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7152</th>\n",
       "      <td>away</td>\n",
       "      <td>don</td>\n",
       "      <td>-0.257455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12162</th>\n",
       "      <td>basically</td>\n",
       "      <td>directions</td>\n",
       "      <td>-0.258309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20080</th>\n",
       "      <td>boat</td>\n",
       "      <td>able</td>\n",
       "      <td>-0.259107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133043</th>\n",
       "      <td>making</td>\n",
       "      <td>attached</td>\n",
       "      <td>-0.259578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135664</th>\n",
       "      <td>maybe</td>\n",
       "      <td>don</td>\n",
       "      <td>-0.260346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154268</th>\n",
       "      <td>parts</td>\n",
       "      <td>finally</td>\n",
       "      <td>-0.262802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34942</th>\n",
       "      <td>children</td>\n",
       "      <td>paid</td>\n",
       "      <td>-0.264020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15551</th>\n",
       "      <td>better</td>\n",
       "      <td>worked</td>\n",
       "      <td>-0.264369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247519</th>\n",
       "      <td>works</td>\n",
       "      <td>bigger</td>\n",
       "      <td>-0.265000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203053</th>\n",
       "      <td>son</td>\n",
       "      <td>like</td>\n",
       "      <td>-0.266562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212471</th>\n",
       "      <td>straight</td>\n",
       "      <td>do</td>\n",
       "      <td>-0.268594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62272</th>\n",
       "      <td>don</td>\n",
       "      <td>basically</td>\n",
       "      <td>-0.270312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34712</th>\n",
       "      <td>children</td>\n",
       "      <td>close</td>\n",
       "      <td>-0.272175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51927</th>\n",
       "      <td>definitely</td>\n",
       "      <td>isn</td>\n",
       "      <td>-0.273339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123368</th>\n",
       "      <td>like</td>\n",
       "      <td>saw</td>\n",
       "      <td>-0.273754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37224</th>\n",
       "      <td>close</td>\n",
       "      <td>colors</td>\n",
       "      <td>-0.278408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236324</th>\n",
       "      <td>using</td>\n",
       "      <td>seams</td>\n",
       "      <td>-0.279679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62402</th>\n",
       "      <td>don</td>\n",
       "      <td>finally</td>\n",
       "      <td>-0.281187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16851</th>\n",
       "      <td>bigger</td>\n",
       "      <td>needs</td>\n",
       "      <td>-0.288639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203429</th>\n",
       "      <td>soon</td>\n",
       "      <td>doesn</td>\n",
       "      <td>-0.289033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62708</th>\n",
       "      <td>don</td>\n",
       "      <td>twice</td>\n",
       "      <td>-0.298328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7004</th>\n",
       "      <td>attached</td>\n",
       "      <td>water</td>\n",
       "      <td>-0.300492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123182</th>\n",
       "      <td>like</td>\n",
       "      <td>hands</td>\n",
       "      <td>-0.310802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125556 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word1       word2     score\n",
       "108082       inches     figures  0.833207\n",
       "76607       figures       packs  0.825721\n",
       "211253         stop       start  0.816715\n",
       "48523       damaged      popped  0.812956\n",
       "108233       inches       packs  0.812329\n",
       "32093       charged        used  0.811088\n",
       "158221     pictured     cracked  0.810884\n",
       "227883        tried      wasted  0.809038\n",
       "227873        tried        used  0.806167\n",
       "112469        items    balloons  0.804584\n",
       "241741         week       month  0.804216\n",
       "31722       charged     damaged  0.802467\n",
       "147409       opened      played  0.798489\n",
       "227729        tried      played  0.798329\n",
       "140156        month         day  0.797825\n",
       "227778        tried      ripped  0.796584\n",
       "48491       damaged  overpriced  0.796558\n",
       "227721        tried    pictured  0.793427\n",
       "106552  immediately      easily  0.792001\n",
       "45745       cracked     charged  0.788275\n",
       "31919       charged      opened  0.788022\n",
       "211756      stopped     started  0.787957\n",
       "52773     described     charged  0.786303\n",
       "227699        tried      opened  0.782267\n",
       "16968        bigger     smaller  0.779627\n",
       "158193     pictured     charged  0.778561\n",
       "234727         used      opened  0.778471\n",
       "218158     terrible         old  0.777368\n",
       "181787     replaced     charged  0.775764\n",
       "140557        month        year  0.775223\n",
       "...             ...         ...       ...\n",
       "3481            ago        uses -0.251178\n",
       "217435         tell    children -0.253195\n",
       "236443           ve  absolutely -0.253766\n",
       "62050       dollars        paid -0.253809\n",
       "7147           away       doesn -0.254330\n",
       "62442           don        hard -0.254766\n",
       "118596        later         don -0.255086\n",
       "7152           away         don -0.257455\n",
       "12162     basically  directions -0.258309\n",
       "20080          boat        able -0.259107\n",
       "133043       making    attached -0.259578\n",
       "135664        maybe         don -0.260346\n",
       "154268        parts     finally -0.262802\n",
       "34942      children        paid -0.264020\n",
       "15551        better      worked -0.264369\n",
       "247519        works      bigger -0.265000\n",
       "203053          son        like -0.266562\n",
       "212471     straight          do -0.268594\n",
       "62272           don   basically -0.270312\n",
       "34712      children       close -0.272175\n",
       "51927    definitely         isn -0.273339\n",
       "123368         like         saw -0.273754\n",
       "37224         close      colors -0.278408\n",
       "236324        using       seams -0.279679\n",
       "62402           don     finally -0.281187\n",
       "16851        bigger       needs -0.288639\n",
       "203429         soon       doesn -0.289033\n",
       "62708           don       twice -0.298328\n",
       "7004       attached       water -0.300492\n",
       "123182         like       hands -0.310802\n",
       "\n",
       "[125556 rows x 3 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similar words\n",
    "similarities[similarities[\"score\"] < 1].sort_values(\n",
    "    by=\"score\", ascending=False).drop_duplicates(\n",
    "    subset=\"score\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/stats/stats.py:1706: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1279ea320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "vector = nlp(u'banana').vector\n",
    "\n",
    "ax = sns.distplot(vector, kde=False, rug=True)\n",
    "t = ax.set_title('Histogram of Feature Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques\n",
    "\n",
    "### Subsampling\n",
    "\n",
    "What do we do with highly frequent words like `the` or `of`? We don't gain a ton of meaning from training on these words, and they become computationally expensive since they appear so frequently:\n",
    "\n",
    "![alt text](images/subsampling.png \"http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\")\n",
    "In the image above, $z(w_i)$ is the frequency of that particular word divided by the total number of words in the entire corpus. For instance, if a corpus of text has 50 words, and the word `dog` appears 3 times, $z(w_{dog}) = 0.06$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120b5ea58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# write subsampling function\n",
    "def subsample(z):\n",
    "    return ((z * 1000) ** 0.5 + 1) * (0.001 / z)\n",
    "\n",
    "# plot this function:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Z = list(np.linspace(0,1,100))\n",
    "probability_of_keeping = list(map( lambda z: subsample(z), Z))\n",
    "\n",
    "plt.scatter(Z, probability_of_keeping)\n",
    "plt.xlabel(\"Frequency word appears in corpus\")\n",
    "plt.ylabel(\"Probability of keeping\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling\n",
    "\n",
    "If you have 50,000 words in your vocabulary, you need to make 2 x 50,000 updates to your model for each context word for each target word! This is an incredibly costly calculation. For the most part, we don't need to make frequent updates to the majority of the corpus. For instance, if our context word is `midterm` and our target word is `study`, do we really need to speed CPU time computing the gradients for `elephant`? \n",
    "\n",
    "In practice, we will only sample 4-5 negative samples (where the target output is 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Word Embeddings\n",
    "\n",
    "#### How to handle **Out Of Vocabulary (OOV)** words?\n",
    "Although **word2vec** and **FastText** include a significant vocabulary size, there will inevitably be words that are not included. For instance, if you are analyzing text conversations using word embeddings pretrained on Wikipedia text (which typically has more formal vocabulary than everyday language), how will you account for the following words?\n",
    "\n",
    "- DM\n",
    "- ROFLMAO\n",
    "- bae\n",
    "- 😃\n",
    "- #10YearChallenge\n",
    "- wut\n",
    "\n",
    "#### Potential solution: use word embeddings if they are available, and otherwise initialize the weights to random.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "def vectorize_word(input_word: str, D=50):\n",
    "    \"\"\"\n",
    "    D: an integer that represents the length (dimensionality of the word embeddings)\n",
    "    word_embeddings: A dictionary object with the string word as the key, and the embedding vector of \n",
    "    length D as the values.\n",
    "    For instance, word_embeddings[\"cat\"] will return [2.3, 4.5, 6.1, -2.2, ...]\n",
    "    \"\"\"\n",
    "    if input_word in word_embeddings.keys():\n",
    "        return word_embeddings[input_word]\n",
    "    else:\n",
    "        return np.random.rand(D)\n",
    "```\n",
    "\n",
    "##### Should we update the word embedding matrices during the model training step?\n",
    "- Ideally, you'd only want to be able to update the specific weights that were randomly initialized (since the rest of the weights are by definition pre-trained and are already pretty good). However, most deep learning libraries do not allow you to easily select which specific weight elements to apply backpropagation to- you either update all weights or you update none. In practice, most data scientists will \"freeze\" the word embedding layer:\n",
    "\n",
    "In Keras:\n",
    "```python\n",
    "word_embedding_layer.trainable = False # by default, trainable is set to true in Keras\n",
    "```\n",
    "In Tensorflow:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "N = 300 # number of words\n",
    "D = 50 # of dimensions in embeddings\n",
    "initial_word_embeddings = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "tensor = tf.constant(initial_word_embeddings, shape=[N, D])\n",
    "```\n",
    "\n",
    "- Ambiguity around **Domain-specific words**: using a generic pre-trained word embedding will not capture the semantic meaning of the word **sack** when it is used in the context of American football:\n",
    "![sack](images/football-bag-sack-diff.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
